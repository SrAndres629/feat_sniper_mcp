import pandas as pd
import numpy as np
from app.core.config import settings

def _smma(series: pd.Series, period: int) -> pd.Series:
    """
    Smoothed Moving Average (SMMA).
    Formula: SMMA(i) = (Sum - SMMA(i-1) + Close(i)) / period
    """
    return series.ewm(alpha=1/period, adjust=False).mean()

def robust_scaler(value: float, median: float, iqr: float) -> float:
    """
    NormalizaciÃ³n Robusta: (x - mediana) / IQR.
    Inmune a outliers de volatilidad extrema.
    """
    if iqr == 0: return 0.0
    return (value - median) / iqr

def calculate_feat_layers(df: pd.DataFrame) -> pd.DataFrame:
    """
    [P0-4 FIX] Calcula las capas FEAT con hot-path 100% NumPy.
    
    Latencia objetivo: < 0.5ms (antes: 2-8ms con Pandas EWM)
    
    Optimizaciones aplicadas:
    - ewm_numpy() en lugar de pd.ewm()
    - Operaciones vectorizadas puras
    - Eliminado pd.Series.pct_change() por np.diff
    """
    if df.empty or len(df) < settings.LAYER_BIAS_PERIOD:
        return pd.DataFrame()

    # Extract numpy arrays for hot-path (single allocation)
    window = 50
    tail_df = df.tail(settings.LAYER_BIAS_PERIOD + window)
    close_arr = tail_df['close'].values.astype(np.float64)
    n = len(close_arr)
    
    # =========================================================================
    # [P0-4] Layer 1 (Micro) - Pure NumPy EWM
    # =========================================================================
    l1_data = np.zeros((n, len(settings.LAYER_MICRO_PERIODS)))
    for i, p in enumerate(settings.LAYER_MICRO_PERIODS):
        alpha = 1.0 / p
        l1_data[:, i] = ewm_numpy(close_arr, alpha)
    
    # =========================================================================
    # [P0-4] Layer 2 (Operative) - Pure NumPy EWM
    # =========================================================================
    l2_data = np.zeros((n, len(settings.LAYER_OPERATIVE_PERIODS)))
    for i, p in enumerate(settings.LAYER_OPERATIVE_PERIODS):
        alpha = 1.0 / p
        l2_data[:, i] = ewm_numpy(close_arr, alpha)
    
    # =========================================================================
    # [P0-4] Layer 4 (Bias/Gravity) - Pure NumPy
    # =========================================================================
    l4_ewm = ewm_numpy(close_arr, 1.0 / settings.LAYER_BIAS_PERIOD)
    
    # L4 Slope: percentage change over 3 periods (replaces pd.pct_change)
    l4_slope = np.zeros(n)
    if n > 3:
        # pct_change(periods=3) = (current - past) / past * 100
        l4_slope[3:] = (l4_ewm[3:] - l4_ewm[:-3]) / np.maximum(np.abs(l4_ewm[:-3]), 1e-10) * 100
    
    # =========================================================================
    # Construct Output - Minimal DataFrame creation
    # =========================================================================
    l1_mean = np.mean(l1_data, axis=1)
    l1_width = np.std(l1_data, axis=1)
    l2_mean = np.mean(l2_data, axis=1)
    div_l1_l2 = l1_mean / np.maximum(l2_mean, 1e-6)
    
    # Volume Z-Score (if available)
    vol_zscore = np.zeros(n)
    if 'volume' in tail_df.columns or 'tick_volume' in tail_df.columns:
        vol_col = 'volume' if 'volume' in tail_df.columns else 'tick_volume'
        vol_arr = tail_df[vol_col].values.astype(np.float64)
        if n >= 100:
            # Rolling mean and std using numpy (last 100 values)
            vol_mean = np.convolve(vol_arr, np.ones(100)/100, mode='same')
            # Rolling std approximation
            vol_squared = np.convolve(vol_arr**2, np.ones(100)/100, mode='same')
            vol_std = np.sqrt(np.maximum(vol_squared - vol_mean**2, 0))
            vol_std = np.where(vol_std == 0, 1, vol_std)  # Avoid division by zero
            vol_zscore = (vol_arr - vol_mean) / vol_std
    
    # Final DataFrame construction (only at the end, not in hot-path)
    res_df = pd.DataFrame({
        'L1_Mean': l1_mean,
        'L1_Width': l1_width,
        'L4_Slope': l4_slope,
        'Div_L1_L2': div_l1_l2,
        'close': close_arr,
        'Vol_ZScore': vol_zscore
    }, index=tail_df.index)
    
    return res_df.tail(window).fillna(0.0)

def detect_regime(physics_df: pd.DataFrame) -> str:
    """
    Modular Engineering: Regime Detection Logic.
    """
    if physics_df.empty:
        return "NO_DATA"
        
    latest = physics_df.iloc[-1]
    
    # 1. Compression Check (Volatility Alert)
    rolling_width = physics_df['L1_Width'].rolling(100).mean().iloc[-1]
    if latest['L1_Width'] < rolling_width * 0.5:
        return "COMPRESSION"
        
    # 2. Expansion Check (Structural Stretch)
    if latest['Div_L1_L2'] > 1.05 or latest['Div_L1_L2'] < 0.95:
        return "EXPANSION"
        
    # 3. Gravity Check (Macro Trend)
    if abs(latest['L4_Slope']) > 0.05:
        return "TREND_GRAVITY"
        
    return "NOISE"

def calculate_atr(df: pd.DataFrame, period: int = 14) -> float:
    """
    Calculates Average True Range (ATR) for volatility measurement.
    NOTE: For hot-path usage, prefer calculate_atr_numpy() below.
    """
    if df.empty or len(df) < period:
        return 0.0
        
    high_low = df['high'] - df['low']
    high_cp = np.abs(df['high'] - df['close'].shift())
    low_cp = np.abs(df['low'] - df['close'].shift())
    tr = pd.concat([high_low, high_cp, low_cp], axis=1).max(axis=1)
    
    atr = tr.rolling(period).mean().iloc[-1]
    return float(atr) if not np.isnan(atr) else 0.0


# =============================================================================
# NUMPY HOT-PATH FUNCTIONS (Senior Patch v9.0)
# These functions avoid Pandas overhead for sub-1ms latency.
# =============================================================================

def calculate_atr_numpy(high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 14) -> float:
    """
    [HOT-PATH] Pure NumPy ATR calculation - 10-50x faster than Pandas version.
    
    Args:
        high: High prices as numpy array
        low: Low prices as numpy array
        close: Close prices as numpy array
        period: ATR period (default 14)
        
    Returns:
        float: ATR value
    """
    n = len(close)
    if n < period + 1:
        return 0.0
    
    # True Range components (avoid slice copy with views)
    high_low = high[1:] - low[1:]
    high_cp = np.abs(high[1:] - close[:-1])
    low_cp = np.abs(low[1:] - close[:-1])
    
    # True Range = max of the three
    tr = np.maximum(np.maximum(high_low, high_cp), low_cp)
    
    # Simple rolling mean of last 'period' values
    atr = np.mean(tr[-period:])
    return float(atr) if not np.isnan(atr) else 0.0


def ewm_numpy(arr: np.ndarray, alpha: float) -> np.ndarray:
    """
    [HOT-PATH] Pure NumPy EWM - avoids Pandas Series allocation.
    
    Replicates pd.Series.ewm(alpha=alpha, adjust=False).mean()
    """
    n = len(arr)
    if n == 0:
        return np.array([])
    
    result = np.zeros(n)
    result[0] = arr[0]
    
    for i in range(1, n):
        result[i] = alpha * arr[i] + (1 - alpha) * result[i - 1]
    
    return result
def detect_divergence(df: pd.DataFrame, window: int = 10) -> str:
    """
    [PVP ALPHA] Detects Liquidity Traps via divergence between Price and L4_Slope.
    L4_Slope represents 'Physics Gravity'. If Price rises but Gravity falls, it's a Trap.
    """
    if df.empty or len(df) < window + 2:
        return "NEUTRAL"
        
    recent = df.tail(window)
    price_change = recent['close'].iloc[-1] - recent['close'].iloc[0]
    slope_change = recent['L4_Slope'].iloc[-1] - recent['L4_Slope'].iloc[0]
    
    # 1. Bearish Divergence (Retail Trap / Top)
    if price_change > 0 and slope_change < -0.01:
        return "BULL_TRAP"
        
    # 2. Bullish Divergence (Retail Trap / Bottom)
    if price_change < 0 and slope_change > 0.01:
        return "BEAR_TRAP"
        
    return "NEUTRAL"
