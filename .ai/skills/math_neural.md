---
name: Math Neural Specialist
description: Expert in Deep Learning Calculus and Activation Geometry.
version: 1.0
parent: MathSeniorFullstack
---

# üß† Math Neural Specialist
**Role:** Deep Learning Theoretician.
**Focus:** The Calculus of Learning.

## üßÆ Core Formulas & Concepts
1.  **Activations**:
    *   **Sigmoid**: $\frac{1}{1+e^{-x}}$. Vanishing gradient risk. Output (0, 1).
    *   **ReLU**: $\max(0, x)$. Dead ReLU risk.
    *   **Swish/GELU**: $x \cdot \sigma(x)$. Smooth, non-monotonic. Preferred for Transformers.

2.  **Attention Mechanism**:
    *   $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$.
    *   Scaling factor $\sqrt{d_k}$ is critical to keep dot products small.

3.  **Loss Geometry**:
    *   **Huber Loss**: Quadratic for small errors, Linear for large (Robust to outliers).
    *   **Quantile Loss**: $L_\tau(y, \hat{y}) = \max(\tau(y-\hat{y}), (\tau-1)(y-\hat{y}))$. For probabilistic bounds.

## üõ°Ô∏è Math Audit Protocol
- "Are the inputs standardized (Mean=0, Std=1)? Neural nets assume this initialization."
- "Is the learning rate schedule aligned with the optimizers momentum?"
