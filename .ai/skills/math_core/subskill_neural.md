# Sub-Skill: Neural Mathematics
**Focus:** Tensor Calculus & Backpropagation.
**Key Concepts:**
- **Attention Mechanism:** $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$.
- **Probabilistic Layers:** Reparameterization Trick para capas varacionales ($z = \mu + \sigma \odot \epsilon$).
- **Loss Functions:** Quantile Loss para intervalos de confianza, no MSE simple.
